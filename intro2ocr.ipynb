{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dangc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dangc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from RealESRGAN import RealESRGAN\n",
    "import pytesseract\n",
    "from pytesseract import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sample_data/box_toy.jpg')\n",
    "# img = np.rot90(img)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Page segmentation modes available in Tesseract:\n",
    "\n",
    "0: Orientation and script detection (Orientation and Script Detection-OSD) only.\n",
    "\n",
    "1: Automatic page segmentation with OSD.\n",
    "\n",
    "2: Automatic page segmentation, but no OSD, or OCR (optical character recognition).\n",
    "\n",
    "3: Fully automatic page segmentation, but no OSD. (Default)\n",
    "\n",
    "4: Assume a single column of text of variable sizes.\n",
    "\n",
    "5: Assume a single uniform block of vertically aligned text.\n",
    "\n",
    "6: Assume a single uniform block of text.\n",
    "\n",
    "7: Treat the image as a single text line.\n",
    "\n",
    "8: Treat the image as a single word.\n",
    "\n",
    "9: Treat the image as a single word in a circle.\n",
    "\n",
    "10: Treat the image as a single character.\n",
    "\n",
    "11: Sparse text. Find as much text as possible in no particular order.\n",
    "\n",
    "12: Sparse text with OSD.\n",
    "\n",
    "13: Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "- OCR Engine Modes:\n",
    "\n",
    "OEM 0: Legacy engine only\n",
    "\n",
    "OEM 1: Neural nets LSTM engine only\n",
    "\n",
    "OEM 2: Legacy + LSTM engines\n",
    "\n",
    "OEM 3: Default, based on what is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download pretrained weights for RealESRGAN there: https://drive.google.com/drive/folders/16PlVKhTNkSyWFx52RPb2hXPIQveNGbxS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RealESRGAN('cpu', scale=4)\n",
    "model.load_weights('weights/RealESRGAN_x4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr_image = model.predict(img)\n",
    "# plt.imshow(sr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xÃ¡m hÃ³a áº£nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 832)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'ae OE\\n\\ny =\\n\\nâ€”\\n\\na Â®\\n\\n=>\\n\\n9/10\\n\\nA\\n\\n-\\n\\n5)\\n\\nASTON MARTIN V12 SPEEDSTER\\n\\nto\\n'}\n"
     ]
    }
   ],
   "source": [
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "# path = 'C:\\Program Files\\Tesseract-OCR\\tessdata'\n",
    "config = rf' --psm 11 --oem 3'\n",
    "img = cv2.imread('sample_data/box_toy.jpg')\n",
    "print(pytesseract.image_to_string(img, config=config, output_type=Output.DICT))\n",
    "data = pytesseract.image_to_data(img, config=config, output_type=Output.DICT)\n",
    "\n",
    "n_boxes = len(data['text'])\n",
    "for i in range(n_boxes):\n",
    "    if float(data['conf'][i]) > 0:\n",
    "        x, y, w, h = int(data['left'][i]), int(data['top'][i]), int(data['width'][i]), int(data['height'][i])\n",
    "        img_r = cv2.rectangle(img, (x,y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        img_r = cv2.putText(img_r, data['text'][i], (x, y+h+20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.7, (255,0,0), 2)\n",
    "\n",
    "output_image = Image.fromarray(img_r)\n",
    "output_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TrOCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dangc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dangc\\.cache\\huggingface\\hub\\models--microsoft--trocr-base-printed. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\dangc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open(\"sample_data/box_toy.jpg\")\n",
    "img1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = processor(images=img1, return_tensors=\"pt\").pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: ['PAX']\n"
     ]
    }
   ],
   "source": [
    "# Generate text from the image\n",
    "generated_ids = model.generate(pixel_values)\n",
    "predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Print the predicted text\n",
    "print(\"Predicted Text:\", predicted_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
